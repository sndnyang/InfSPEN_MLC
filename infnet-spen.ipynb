{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05351278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3685a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e0bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f990f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12adad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e089046",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bibtex/train.pickle', \"rb\") as f:\n",
    "    temp = pickle.load(f)\n",
    "    data_x = np.array([instance['feats'] for instance in temp])\n",
    "    data_y = np.array([instance['types'] for instance in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42a9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bibtex/test.pickle', \"rb\") as f:\n",
    "    temp = pickle.load(f)\n",
    "    test_x = np.array([instance['feats'] for instance in temp])\n",
    "    test_y = np.array([instance['types'] for instance in temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb46a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafbbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = torch.FloatTensor(data_x).to(device)\n",
    "data_y = torch.FloatTensor(data_y).to(device)\n",
    "test_x = torch.FloatTensor(test_x).to(device)\n",
    "test_y = torch.FloatTensor(test_y).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722631a8",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e44def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1836, 150)\n",
    "        self.layer2 = nn.Linear(150, 150)\n",
    "        self.out_l = nn.Linear(150, 159)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, only_feature_extraction=False):\n",
    "        out = func.relu(self.layer1(x))\n",
    "        out = func.relu(self.layer2(out))\n",
    "        if not only_feature_extraction:\n",
    "            out = self.out_l(out)\n",
    "            out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ef67c",
   "metadata": {},
   "source": [
    "# f1 and mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d73079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold = [0.05, 0.10, 0.15, 0.2, 0.25, 0.30, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.70, 0.75]\n",
    "def f1_map(test_y, pred_test):\n",
    "    best_f1 = 0\n",
    "    for t in Threshold:\n",
    "        pred = pred_test > t\n",
    "        f1 = f1_score(test_y.data.cpu().numpy(), pred.data.cpu().numpy(), average='samples')\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "    mAP = np.mean(metrics.average_precision_score(\n",
    "        test_y.data.cpu().numpy(), pred_test.data.cpu().numpy(), average=None\n",
    "    ))\n",
    "    \n",
    "    return best_f1, mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6d70e",
   "metadata": {},
   "source": [
    "# pre-training feature net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1a0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ac738c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0.0414399394039205 0.020842633841436933\n",
      "\n",
      "epoch 1\n",
      "0.05286597873078986 0.02799239158708507\n",
      "\n",
      "epoch 2\n",
      "0.07068036227081953 0.052707762726149784\n",
      "\n",
      "epoch 3\n",
      "0.1673552778125343 0.09121999194169501\n",
      "\n",
      "epoch 4\n",
      "0.21151587312621106 0.118025021028019\n",
      "\n",
      "epoch 5\n",
      "0.23361619447249293 0.14419005436022742\n",
      "\n",
      "epoch 6\n",
      "0.26242346012723744 0.1692342302364163\n",
      "\n",
      "epoch 7\n",
      "0.29072475836716877 0.1967457966908786\n",
      "\n",
      "epoch 8\n",
      "0.3097807762073427 0.21543534492927094\n",
      "\n",
      "epoch 9\n",
      "0.3349576502623345 0.23439222164592144\n",
      "\n",
      "epoch 10\n",
      "0.34892444884855606 0.25013439869614496\n",
      "\n",
      "epoch 11\n",
      "0.35706535406406764 0.2622146933649457\n",
      "\n",
      "epoch 12\n",
      "0.3628598005731947 0.27239799682330157\n",
      "\n",
      "epoch 13\n",
      "0.37195683954212655 0.27919367332978173\n",
      "\n",
      "epoch 14\n",
      "0.37369825977039467 0.28451671429981823\n",
      "\n",
      "epoch 15\n",
      "0.37713901792080695 0.2898621354885521\n",
      "\n",
      "epoch 16\n",
      "0.3822975290311186 0.2924142790193828\n",
      "\n",
      "epoch 17\n",
      "0.3834344100502664 0.293635805789803\n",
      "\n",
      "epoch 18\n",
      "0.39442181802600956 0.2970049204138833\n",
      "\n",
      "epoch 19\n",
      "0.38807548450631135 0.2973065234538496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(20):\n",
    "    inds = np.random.permutation(list(range(len(data_x))))\n",
    "    for i in range(0, 4880, 80):\n",
    "        l = inds[i:i+80]\n",
    "        data = data_x[l]\n",
    "        label = data_y[l]\n",
    "        logits = model(data)\n",
    "        \n",
    "        loss = criterion(logits, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch', epoch)\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_x)\n",
    "        pred_test = torch.sigmoid(logits)\n",
    "    \n",
    "    best_f1, mAP = f1_map(test_y, pred_test)\n",
    "    print(best_f1, mAP)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d651158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38807548450631135 0.2973065234538496\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(test_x)\n",
    "    pred_test = torch.sigmoid(logits)\n",
    "\n",
    "best_f1, mAP = f1_map(test_y, pred_test)\n",
    "print(best_f1, mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67adbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyNetwork(nn.Module):\n",
    "    def __init__(self, weights_last_layer_mlp=150, feature_dim=150, label_dim=159,\n",
    "                 num_pairwise=16, non_linearity=nn.Softplus()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.non_linearity = non_linearity\n",
    "\n",
    "        self.B = torch.nn.Parameter(torch.transpose(-weights_last_layer_mlp, 0, 1))\n",
    "\n",
    "        # Label energy terms, C1/c2  in equation 5 of SPEN paper\n",
    "        self.C1 = torch.nn.Parameter(torch.empty(label_dim, num_pairwise))\n",
    "        torch.nn.init.normal_(self.C1, mean=0, std=np.sqrt(2.0 / label_dim))\n",
    "\n",
    "        self.c2 = torch.nn.Parameter(torch.empty(num_pairwise, 1))\n",
    "        torch.nn.init.normal_(self.c2, mean=0, std=np.sqrt(2.0 / num_pairwise))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Local energy\n",
    "        e_local = torch.mm(x, self.B)\n",
    "        # element-wise product\n",
    "        e_local = torch.mul(y, e_local)\n",
    "        e_local = torch.sum(e_local, dim=1)\n",
    "        e_local = e_local.view(e_local.size()[0], 1)\n",
    "\n",
    "        # Label energy\n",
    "        e_label = self.non_linearity(torch.mm(y, self.C1))\n",
    "        e_label = torch.mm(e_label, self.c2)\n",
    "        e_global = torch.add(e_label, e_local)\n",
    "\n",
    "        return e_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0ff69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class SPEN():\n",
    "    def __init__(self, feature_net, energy_net, inf_net, n_steps_inf=1, input_dim=1836, label_dim=159, num_pairwise=16,\n",
    "                 learning_rate=1e-5, weight_decay=1e-4, non_linearity=nn.Softplus()):\n",
    "        self.feature_extractor = feature_net\n",
    "        self.feature_extractor.eval()\n",
    "        self.energy_net = energy_net\n",
    "        self.inf_net = inf_net\n",
    "        self.loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def _compute_energy(self, inputs, targets):\n",
    "        f_x = self.feature_extractor(inputs, only_feature_extraction=True)\n",
    "        \n",
    "        # Energy ground truth\n",
    "        gt_energy = self.energy_net(f_x, targets)\n",
    "        \n",
    "        # Cost-augmented inference network\n",
    "        pred_labels = self.inf_net(inputs)\n",
    "        pred_energy = self.energy_net(f_x, pred_labels)\n",
    "        return pred_labels, pred_energy, gt_energy\n",
    "    \n",
    "    def compute_loss(self, inputs, targets):\n",
    "        \n",
    "        pred_labels, pred_energy, gt_energy = self._compute_energy(inputs, targets)\n",
    "        # Max-margin Loss\n",
    "        pre_loss = self.loss_fn(pred_labels, targets) - pred_energy + gt_energy\n",
    "        eneryg_loss = torch.max(pre_loss, torch.zeros_like(pre_loss))\n",
    "        eneryg_loss = torch.mean(eneryg_loss)        \n",
    "        inf_net_loss = eneryg_loss\n",
    "        e_net_loss = eneryg_loss + 0.01 * sum(p.pow(2.0).sum() for p in self.energy_net.parameters())\n",
    "        return pred_labels, e_net_loss, inf_net_loss\n",
    "\n",
    "    def pred(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.inf_net(x)\n",
    "        return y_pred\n",
    "    \n",
    "    def inference(self, x, training=False, n_steps=1):\n",
    "        \n",
    "        sd = self.inf_net.state_dict()\n",
    "        inf_net2 = MLP()\n",
    "        inf_net2.load_state_dict(sd)\n",
    "        self.inf_net.eval()\n",
    "        optimizer = optim.SGD(self.inf_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.inf_net(x)\n",
    "        \n",
    "        self.inf_net.train()\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a20581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "eps = 1e-8\n",
    "class SPEN():\n",
    "    def __init__(self, feature_net, energy_net, inf_net, n_steps_inf=1, input_dim=1836, label_dim=159, num_pairwise=16,\n",
    "                 learning_rate=1e-5, weight_decay=1e-4, non_linearity=nn.Softplus()):\n",
    "        self.feature_extractor = feature_net\n",
    "        self.feature_extractor.eval()\n",
    "        self.energy_net = energy_net\n",
    "        self.inf_net = inf_net\n",
    "        self.loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def _compute_energy(self, inputs, targets):\n",
    "        f_x = self.feature_extractor(inputs, only_feature_extraction=True)\n",
    "        \n",
    "        # Energy ground truth\n",
    "        gt_energy = self.energy_net(f_x, targets)\n",
    "        \n",
    "        # Cost-augmented inference network\n",
    "        pred_labels = self.inf_net(inputs)\n",
    "        pred_energy = self.energy_net(f_x, pred_labels)\n",
    "        return pred_labels, pred_energy, gt_energy\n",
    "    \n",
    "    def compute_loss(self, inputs, targets):\n",
    "        \n",
    "        pred_labels, pred_energy, gt_energy = self._compute_energy(inputs, targets)\n",
    "        # Max-margin Loss\n",
    "        pre_loss = self.loss_fn(pred_labels, targets) - pred_energy + gt_energy\n",
    "        eneryg_loss = torch.max(pre_loss, torch.zeros_like(pre_loss))\n",
    "        eneryg_loss = torch.mean(eneryg_loss)\n",
    "        \n",
    "        pred_y = pred_labels\n",
    "\n",
    "        entropy_loss = - torch.mean(pred_y * torch.log(pred_y) + (1 - pred_y) * torch.log(1 - pred_y))\n",
    "        \n",
    "        inf_net_loss = -eneryg_loss + 0.01 * sum(p.pow(2.0).sum() for p in self.inf_net.parameters()) \\\n",
    "                       + 10 * sum((x - y).pow(2.0).sum() for x, y in zip(self.inf_net.state_dict().values(), self.feature_extractor.state_dict().values())) \\\n",
    "                       + entropy_loss\n",
    "        e_net_loss = eneryg_loss + 0.01 * sum(p.pow(2.0).sum() for p in self.energy_net.parameters())\n",
    "        return pred_labels, e_net_loss, inf_net_loss\n",
    "\n",
    "    def pred(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.inf_net(x)\n",
    "        return y_pred\n",
    "    \n",
    "    def inference(self, x, training=False, n_steps=1):\n",
    "        \n",
    "        sd = self.inf_net.state_dict()\n",
    "        inf_net2 = MLP()\n",
    "        inf_net2.load_state_dict(sd)\n",
    "        self.inf_net.eval()\n",
    "        optimizer = optim.SGD(self.inf_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.inf_net(x)\n",
    "        \n",
    "        self.inf_net.train()\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8ed2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_net = MLP().to(device)\n",
    "energy_net = EnergyNetwork(model.out_l.weight).to(device)\n",
    "\n",
    "optim_inf = torch.optim.Adam(inf_net.parameters(), lr=3e-4, weight_decay=0)\n",
    "optim_energy = torch.optim.Adam(energy_net.parameters(), lr=1e-5, weight_decay=0)\n",
    "\n",
    "spen = SPEN(model, energy_net, inf_net, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed190aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0dab9f2e807a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mbest_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-56fd5b1ec876>\u001b[0m in \u001b[0;36mf1_map\u001b[0;34m(test_y, pred_test)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     mAP = np.mean(metrics.average_precision_score(\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     ))\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/research/audioclf/venv/lib64/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/research/audioclf/venv/lib64/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36maverage_precision_score\u001b[0;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    223\u001b[0m                                 pos_label=pos_label)\n\u001b[1;32m    224\u001b[0m     return _average_binary_score(average_precision, y_true, y_score,\n\u001b[0;32m--> 225\u001b[0;31m                                  average, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/research/audioclf/venv/lib64/python3.6/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mnot_average_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/research/audioclf/venv/lib64/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/research/audioclf/venv/lib64/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 664\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/research/audioclf/venv/lib64/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n\u001b[0;32m--> 106\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    107\u001b[0m             )\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(100):\n",
    "    inds = np.random.permutation(list(range(len(data_x))))\n",
    "    for i in range(0, 4880, 80):\n",
    "        l = inds[i:i+80]\n",
    "        data = data_x[l]\n",
    "        label = data_y[l]\n",
    "        \n",
    "        optim_inf.zero_grad()\n",
    "        preds, e_loss, inf_loss = spen.compute_loss(data, label)\n",
    "        # preds, inf_loss = spen.compute_loss(data, label)\n",
    "        \n",
    "        inf_loss.backward()\n",
    "        optim_inf.step()\n",
    "        \n",
    "        optim_energy.zero_grad()\n",
    "        preds, e_loss, inf_loss = spen.compute_loss(data, label)\n",
    "        \n",
    "        e_loss.backward()\n",
    "        optim_energy.step()\n",
    "    print(epoch)\n",
    "\n",
    "    pred_test = spen.pred(test_x)\n",
    "    best_f1, mAP = f1_map(test_y, pred_test)\n",
    "    print(best_f1, mAP)\n",
    "    \n",
    "    # pred_test = spen.inference(test_x)\n",
    "    # best_f1, mAP = f1_map(test_y, pred_test)\n",
    "    # print(best_f1, mAP)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf0efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}